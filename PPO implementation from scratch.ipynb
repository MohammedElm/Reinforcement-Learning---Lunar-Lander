{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO algorithm Implementation from scratch to solve \"Lunar Lander\" problem (Continuous version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import deque, namedtuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "# Load packages\n",
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import trange\n",
    "import torch.nn as nn\n",
    "from scipy.stats import norm\n",
    "Experience = namedtuple('Experience',\n",
    "                        ['state', 'action', 'reward', 'next_state', 'done'])\n",
    "\n",
    "\n",
    "def running_average(x, N):\n",
    "    ''' Function used to compute the running average\n",
    "        of the last N elements of a vector x\n",
    "    '''\n",
    "    if len(x) >= N:\n",
    "        y = np.copy(x)\n",
    "        y[N-1:] = np.convolve(x, np.ones((N, )) / N, mode='valid')\n",
    "    else:\n",
    "        y = np.zeros_like(x)\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a pytorch-compatible density function for the 1-D distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_density(x, mu, sigma):\n",
    "    \"\"\" Input are tensors\"\"\"\n",
    "    \n",
    "    factor= 1./(torch.sqrt(2*np.pi*sigma))\n",
    "    expo = torch.exp(-((x-mu)**2)/(2*sigma))\n",
    "    return factor*expo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define our main classes (Replay buffer, Networks, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    ''' Base agent class\n",
    "\n",
    "        Args:\n",
    "            n_actions (int): actions dimensionality\n",
    "\n",
    "        Attributes:\n",
    "            n_actions (int): where we store the dimensionality of an action\n",
    "    '''\n",
    "    def __init__(self, n_actions: int):\n",
    "        self.n_actions = n_actions\n",
    "\n",
    "    def forward(self, state: np.ndarray):\n",
    "        ''' Performs a forward computation '''\n",
    "        pass\n",
    "\n",
    "    def backward(self):\n",
    "        ''' Performs a backward pass on the network '''\n",
    "        pass\n",
    "\n",
    "\n",
    "Experience = namedtuple('Experience',\n",
    "                        ['state', 'action', 'reward', 'next_state', 'done'])\n",
    "\n",
    "\n",
    "    \n",
    "class ExperienceReplayBuffer(object):\n",
    "    \"\"\" Class used to store a buffer containing experiences of the RL agent.\n",
    "    \"\"\"\n",
    "    def __init__(self, maximum_length):\n",
    "        # Create buffer of maximum length\n",
    "        self.buffer = deque(maxlen=maximum_length)\n",
    "\n",
    "    def append(self, experience):\n",
    "        # Append experience to the buffer\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def __len__(self):\n",
    "        # overload len operator\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def sample_batch(self, n):\n",
    "        \"\"\" Function used to sample experiences from the buffer.\n",
    "            returns 5 lists, each of size n. Returns a list of state, actions,\n",
    "            rewards, next states and done variables.\n",
    "        \"\"\"\n",
    "        # If we try to sample more elements that what are available from the\n",
    "        # buffer we raise an error\n",
    "        if n > len(self.buffer):\n",
    "            raise IndexError('Tried to sample too many elements from the buffer!')\n",
    "\n",
    "        # Sample without replacement the indices of the experiences\n",
    "        # np.random.choice takes 3 parameters: number of elements of the buffer,\n",
    "        # number of elements to sample and replacement.\n",
    "        indices = np.random.choice(\n",
    "            len(self.buffer),\n",
    "            size=n,\n",
    "            replace=False\n",
    "        )\n",
    "\n",
    "        # Using the indices that we just sampled build a list of chosen experiences\n",
    "        batch = [self.buffer[i] for i in indices]\n",
    "\n",
    "        # batch is a list of size n, where each element is an Experience tuple\n",
    "        # of 5 elements. To convert a list of tuples into\n",
    "        # a tuple of list we do zip(*batch). In this case this will return a\n",
    "        # tuple of 5 elements where each element is a list of n elements.\n",
    "        return zip(*batch)\n",
    "\n",
    "### Neural Network ###\n",
    "class actor_net(nn.Module):\n",
    "    \"\"\" Create a feedforward neural network \"\"\"\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "\n",
    "        # Create input layer with ReLU activation\n",
    "        self.input_layer = nn.Linear(input_size, 400) #Shared Input Layer\n",
    "        \n",
    "        self.input_layer_activation = nn.ReLU()\n",
    "        \n",
    "        self.input_mean = nn.Linear(400, 200)\n",
    "        self.output_mean= nn.Linear(200, output_size)\n",
    "        self.act_mean = nn.Tanh()\n",
    "        \n",
    "        self.input_sigma= nn.Linear(400,200)\n",
    "        self.output_sigma= nn.Linear(200, output_size)\n",
    "        self.act_sigma= nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Function used to compute the forward pass\n",
    "\n",
    "        # Compute first layer\n",
    "        l1 = self.input_layer(x)\n",
    "        l1= self.input_layer_activation(l1)\n",
    "        \n",
    "        #Head mean\n",
    "        mean= self.input_mean(l1)\n",
    "        mean= self.input_layer_activation(mean)\n",
    "        mean= self.output_mean (mean)\n",
    "        mean= self.act_mean(mean)\n",
    "        \n",
    "        #Head sigma\n",
    "        sigma= self.input_sigma(l1)\n",
    "        sigma= self.input_layer_activation(sigma)\n",
    "        sigma= self.output_sigma(sigma)\n",
    "        sigma= self.act_sigma(sigma)\n",
    "        \n",
    "        return mean, sigma\n",
    "    \n",
    "class critic_net(nn.Module):\n",
    "    \"\"\" Create a feedforward neural network \"\"\"\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "\n",
    "        # Create input layer with ReLU activation\n",
    "        self.input_layer = nn.Linear(input_size, 400)\n",
    "        self.input_layer_activation = nn.ReLU()\n",
    "        \n",
    "        self.input_layer1 = nn.Linear(400, 200)\n",
    "        self.output_layer= nn.Linear(200, output_size)\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Function used to compute the forward pass\n",
    "\n",
    "        # Compute first layer\n",
    "        l1 = self.input_layer(x)\n",
    "        l1 = self.input_layer_activation(l1)\n",
    "        l1 = self.input_layer1(l1)\n",
    "        l1 = self.input_layer_activation(l1)\n",
    "        out= self.output_layer(l1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        return out    \n",
    "    \n",
    "    \n",
    "class RandomAgent(Agent):\n",
    "    ''' Agent taking actions uniformly at random, child of the class Agent'''\n",
    "    def __init__(self, n_actions: int):\n",
    "        super(RandomAgent, self).__init__(n_actions)\n",
    "\n",
    "    def forward(self, state: np.ndarray) -> np.ndarray:\n",
    "        ''' Compute a random action in [-1, 1]\n",
    "\n",
    "            Returns:\n",
    "                action (np.ndarray): array of float values containing the\n",
    "                    action. The dimensionality is equal to self.n_actions from\n",
    "                    the parent class Agent\n",
    "        '''\n",
    "        return np.clip(-1 + 2 * np.random.rand(self.n_actions), -1, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we fix our parameters and we launch the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 1337 - Reward/Steps: 264.2/256 - Avg. Reward/Steps: 168.0/247:  84%|█████▊ | 1338/1600 [22:08<01:34,  2.77it/s]"
     ]
    }
   ],
   "source": [
    "# Import and initialize Mountain Car Environment\n",
    "env = gym.make('LunarLanderContinuous-v2')\n",
    "env.reset()\n",
    "\n",
    "# Parameters\n",
    "N_episodes = 1600              # Number of episodes to run for training\n",
    "discount_factor = 0.99        # Value of gamma\n",
    "n_ep_running_average = 50      # Running average of 50 episodes\n",
    "n_actions = len(env.action_space.high)               # Number of available actions\n",
    "dim_state = len(env.observation_space.high)  # State dimensionality\n",
    "epsilon=0.2\n",
    "M=10\n",
    "lr_critic=1e-3\n",
    "lr_actor=1e-5\n",
    "L=10000\n",
    "\n",
    "# Reward\n",
    "episode_reward_list = []  # Used to save episodes reward\n",
    "episode_number_of_steps = []\n",
    "\n",
    "\n",
    "\n",
    "### Filling up Buffer with Random experiences\n",
    "agent = RandomAgent(n_actions)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Create network ###\n",
    "\n",
    "actor = actor_net(input_size= dim_state, output_size=n_actions)\n",
    "critic = critic_net(input_size= dim_state, output_size=1)\n",
    "\n",
    "\n",
    "### Create optimizer ###\n",
    "optimizer_actor = optim.Adam(actor.parameters(), lr=lr_actor)\n",
    "optimizer_critic = optim.Adam(critic.parameters(), lr=lr_critic)\n",
    "\n",
    "\n",
    "# Training process\n",
    "EPISODES = trange(N_episodes, desc='Episode: ', leave=True)\n",
    "\n",
    "for episode in EPISODES:\n",
    "    \n",
    "    ### Create Experience replay buffer for the episode ###\n",
    "    buffer = ExperienceReplayBuffer(maximum_length=L)\n",
    "\n",
    "    total_episode_reward=0\n",
    "    state = env.reset()                    # Reset environment, returns                                     # initial state\n",
    "    done = False                           # Boolean variable used to indicate\n",
    "    t=0                                     #Step of episode \n",
    "    \n",
    "    G=[]\n",
    "    states=[]\n",
    "    actions=[]\n",
    "    \n",
    "    \n",
    "    while not done:\n",
    "        #env.render()                       # Render the environment, remove this\n",
    "                                           # line if you run on Google Colab\n",
    "        # Create state tensor, remember to use single precision (torch.float32)\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            states.append(state)\n",
    "\n",
    "            mu, var = actor(torch.tensor([state]))\n",
    "            mu = mu.detach().numpy()\n",
    "            std = torch.sqrt(var).detach().numpy()\n",
    "            action = np.clip(np.random.normal(mu, std), -1, 1).flatten()            \n",
    "            actions.append(action)\n",
    "\n",
    "        \n",
    "\n",
    "        # The next line takes permits you to take an action in the RL environment\n",
    "        # env.step(action) returns 4 variables:\n",
    "        # (1) next state; (2) reward; (3) done variable; (4) additional stuff\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        \n",
    "        #COMPUTE Relevant measures\n",
    "        G.append(reward)\n",
    "        if len(G)>1:\n",
    "            for i in range(len(G)-1):\n",
    "                G[i]+= (discount_factor**(t-i))*reward\n",
    "\n",
    "        \n",
    "        \n",
    "        # Update episode reward\n",
    "        total_episode_reward += reward\n",
    "\n",
    "        # Append experience to the buffer\n",
    "        exp = Experience(state, action, reward, next_state, done)\n",
    "\n",
    "        buffer.append(exp)\n",
    "        \n",
    "        \n",
    "        # Update state for next iteration\n",
    "        state = next_state\n",
    "\n",
    "        t+=1\n",
    "    \n",
    "    #Compute advantage and old_pi and Psi as targets in order to perform training\n",
    "    G= torch.tensor(G, requires_grad=False, dtype=torch.float32)\n",
    "    states= torch.tensor(states, requires_grad=True, dtype=torch.float32)\n",
    "    actions= torch.tensor(actions, requires_grad=False, dtype=torch.float32)\n",
    "    V= critic(states)\n",
    "    V=torch.reshape(V, G.shape)\n",
    "    Psi= G-V.detach()\n",
    "    means, sigmas= actor(states)\n",
    "    val1, val2= normal_density(actions[:,0], means[:,0], sigmas[:,0]), normal_density(actions[:,1], means[:,1], sigmas[:,1])\n",
    "    pi= val1*val2\n",
    "    old_pi=pi.detach()\n",
    "    \n",
    "    for n in range(1,M+1):\n",
    "        #Optimize Critic\n",
    "        critic.train()\n",
    "        \n",
    "        #Compute Targets\n",
    "    \n",
    "        \n",
    "        V= critic(states)\n",
    "        V=torch.reshape(V, G.shape)\n",
    "        \n",
    "        means, sigmas= actor(states)\n",
    "        val1, val2= normal_density(actions[:,0], means[:,0], sigmas[:,0]), normal_density(actions[:,1], means[:,1], sigmas[:,1])\n",
    "        pi= val1*val2\n",
    "        loss_critic= nn.functional.mse_loss(\n",
    "                            V, G.detach())\n",
    "        \n",
    "        optimizer_critic.zero_grad()\n",
    "            \n",
    "        # Compute gradient\n",
    "        loss_critic.backward()\n",
    "        \n",
    "        nn.utils.clip_grad_norm_(critic.parameters(), max_norm=1.)\n",
    "\n",
    "        # Perform backward pass (backpropagation)\n",
    "        optimizer_critic.step()\n",
    "        \n",
    "        #Optimize Actor\n",
    "        \n",
    "        r= torch.div(pi, old_pi)\n",
    "        \n",
    "        c_eps= torch.max(torch.min(r,torch.tensor(1+epsilon,\n",
    "                                                  requires_grad=False, dtype=torch.float32)), \n",
    "                         torch.tensor(1-epsilon, requires_grad=False, dtype=torch.float32))\n",
    "        \n",
    "        Psi=Psi.detach()\n",
    "        \n",
    "        \n",
    "        actor.train()\n",
    "        \n",
    "        loss_actor= -torch.mean(torch.min(r*Psi,c_eps*Psi))\n",
    "        \n",
    "        optimizer_actor.zero_grad()\n",
    "            \n",
    "        # Compute gradient\n",
    "        loss_actor.backward()\n",
    "        \n",
    "        nn.utils.clip_grad_norm_(actor.parameters(), max_norm=1.)\n",
    "\n",
    "        # Perform backward pass (backpropagation)\n",
    "        optimizer_actor.step()\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Add rewards and number of steps\n",
    "    episode_reward_list.append(total_episode_reward)\n",
    "    episode_number_of_steps.append(t)\n",
    "    EPISODES.set_description(\n",
    "        \"Episode {} - Reward/Steps: {:.1f}/{} - Avg. Reward/Steps: {:.1f}/{}\".format(\n",
    "        episode, total_episode_reward, t,\n",
    "        running_average(episode_reward_list, n_ep_running_average)[-1],\n",
    "        running_average(episode_number_of_steps, n_ep_running_average)[-1]))\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# Close all the windows\n",
    "env.close()\n",
    "\n",
    "\n",
    "# Plot Rewards and steps\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(16, 9))\n",
    "ax[0].plot([i for i in range(1, N_episodes+1)], episode_reward_list, label='Episode reward')\n",
    "ax[0].plot([i for i in range(1, N_episodes+1)], running_average(\n",
    "    episode_reward_list, n_ep_running_average), label='Avg. episode reward')\n",
    "ax[0].set_xlabel('Episodes')\n",
    "ax[0].set_ylabel('Total reward')\n",
    "ax[0].set_title('Total Reward vs Episodes')\n",
    "ax[0].legend()\n",
    "ax[0].grid(alpha=0.3)\n",
    "\n",
    "ax[1].plot([i for i in range(1, N_episodes+1)], episode_number_of_steps, label='Steps per episode')\n",
    "ax[1].plot([i for i in range(1, N_episodes+1)], running_average(\n",
    "    episode_number_of_steps, n_ep_running_average), label='Avg. number of steps per episode')\n",
    "ax[1].set_xlabel('Episodes')\n",
    "ax[1].set_ylabel('Total number of steps')\n",
    "ax[1].set_title('Total number of steps vs Episodes')\n",
    "ax[1].legend()\n",
    "ax[1].grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Solution\n",
    "\n",
    "Here we check the validity of our model. We achieve high performances with the above parameters (200+ average reward over 50 episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 0:   0%|                                                                                | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network model: actor_net(\n",
      "  (input_layer): Linear(in_features=8, out_features=400, bias=True)\n",
      "  (input_layer_activation): ReLU()\n",
      "  (input_mean): Linear(in_features=400, out_features=200, bias=True)\n",
      "  (output_mean): Linear(in_features=200, out_features=2, bias=True)\n",
      "  (act_mean): Tanh()\n",
      "  (input_sigma): Linear(in_features=400, out_features=200, bias=True)\n",
      "  (output_sigma): Linear(in_features=200, out_features=2, bias=True)\n",
      "  (act_sigma): Sigmoid()\n",
      ")\n",
      "Checking solution...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 49: 100%|██████████████████████████████████████████████████████████████████████| 50/50 [00:20<00:00,  2.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy achieves an average total reward of 161.5 +/- 32.5 with confidence 95%.\n",
      "Your policy passed the test!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load packages\n",
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "from tqdm import trange\n",
    "from PPO_agent import actor_net\n",
    "\n",
    "def running_average(x, N):\n",
    "    ''' Function used to compute the running average\n",
    "        of the last N elements of a vector x\n",
    "    '''\n",
    "    if len(x) >= N:\n",
    "        y = np.copy(x)\n",
    "        y[N-1:] = np.convolve(x, np.ones((N, )) / N, mode='valid')\n",
    "    else:\n",
    "        y = np.zeros_like(x)\n",
    "    return y\n",
    "\n",
    "# Load model\n",
    "try:\n",
    "    model = actor\n",
    "    print('Network model: {}'.format(model))\n",
    "except:\n",
    "    print('File neural-network-3-actor.pth not found!')\n",
    "    exit(-1)\n",
    "\n",
    "# Import and initialize Mountain Car Environment\n",
    "env = gym.make('LunarLanderContinuous-v2')\n",
    "env.reset()\n",
    "\n",
    "# Parameters\n",
    "N_EPISODES = 50            # Number of episodes to run for trainings\n",
    "CONFIDENCE_PASS = 125\n",
    "\n",
    "# Reward\n",
    "episode_reward_list = []  # Used to store episodes reward\n",
    "\n",
    "# Simulate episodes\n",
    "print('Checking solution...')\n",
    "EPISODES = trange(N_EPISODES, desc='Episode: ', leave=True)\n",
    "for i in EPISODES:\n",
    "    EPISODES.set_description(\"Episode {}\".format(i))\n",
    "    # Reset enviroment data\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    total_episode_reward = 0.\n",
    "    while not done:\n",
    "        # Get next state and reward.  The done variable\n",
    "        # will be True if you reached the goal position,\n",
    "        # False otherwise\n",
    "        mu, var = model(torch.tensor([state]))\n",
    "        mu = mu.detach().numpy()\n",
    "        std = torch.sqrt(var).detach().numpy()\n",
    "        actions = np.clip(np.random.normal(mu, std), -1, 1).flatten()\n",
    "        next_state, reward, done, _ = env.step(actions)\n",
    "\n",
    "        # Update episode reward\n",
    "        total_episode_reward += reward\n",
    "\n",
    "        # Update state for next iteration\n",
    "        state = next_state\n",
    "\n",
    "    # Append episode reward\n",
    "    episode_reward_list.append(total_episode_reward)\n",
    "\n",
    "    # Close environment\n",
    "    env.close()\n",
    "\n",
    "avg_reward = np.mean(episode_reward_list)\n",
    "confidence = np.std(episode_reward_list) * 1.96 / np.sqrt(N_EPISODES)\n",
    "\n",
    "\n",
    "print('Policy achieves an average total reward of {:.1f} +/- {:.1f} with confidence 95%.'.format(\n",
    "                avg_reward,\n",
    "                confidence))\n",
    "\n",
    "if avg_reward - confidence > CONFIDENCE_PASS:\n",
    "    print('Your policy passed the test!')\n",
    "else:\n",
    "    print(\"Your policy did not pass the test! The average reward of your policy needs to be greater than {} with 95% confidence\".format(CONFIDENCE_PASS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
