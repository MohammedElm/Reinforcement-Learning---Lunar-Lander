{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of DDGP to solve \"Lunar Lander\" Problem (continuous version)\n",
    "\n",
    "We begin by defining useful function and import relevant modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import deque, namedtuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "# Load packages\n",
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import trange\n",
    "from DDPG_soft_updates import soft_updates\n",
    "\n",
    "\n",
    "def running_average(x, N):\n",
    "    ''' Function used to compute the running average\n",
    "        of the last N elements of a vector x\n",
    "    '''\n",
    "    if len(x) >= N:\n",
    "        y = np.copy(x)\n",
    "        y[N-1:] = np.convolve(x, np.ones((N, )) / N, mode='valid')\n",
    "    else:\n",
    "        y = np.zeros_like(x)\n",
    "    return y\n",
    "\n",
    "\n",
    "def soft_updates(network: nn.Module,\n",
    "                 target_network: nn.Module,\n",
    "                 tau: float) -> nn.Module:\n",
    "    \"\"\" Performs a soft copy of the network's parameters to the target\n",
    "        network's parameter\n",
    "\n",
    "        Args:\n",
    "            network (nn.Module): neural network from which we want to copy the\n",
    "                parameters\n",
    "            target_network (nn.Module): network that is being updated\n",
    "            tau (float): time constant that defines the update speed in (0,1)\n",
    "\n",
    "        Returns:\n",
    "            target_network (nn.Module): the target network\n",
    "\n",
    "    \"\"\"\n",
    "    tgt_state = target_network.state_dict()\n",
    "    for k, v in network.state_dict().items():\n",
    "        tgt_state[k] = (1 - tau)  * tgt_state[k]  + tau * v\n",
    "    target_network.load_state_dict(tgt_state)\n",
    "    return target_network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define our Neural Networks structures and the replay buffer class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "Experience = namedtuple('Experience',\n",
    "                        ['state', 'action', 'reward', 'next_state', 'done'])\n",
    "\n",
    "\n",
    "\n",
    "class RandomAgent(Agent):\n",
    "    ''' Agent taking actions uniformly at random, child of the class Agent'''\n",
    "    def __init__(self, n_actions: int):\n",
    "        super(RandomAgent, self).__init__(n_actions)\n",
    "\n",
    "    def forward(self, state: np.ndarray) -> np.ndarray:\n",
    "        ''' Compute a random action in [-1, 1]\n",
    "\n",
    "            Returns:\n",
    "                action (np.ndarray): array of float values containing the\n",
    "                    action. The dimensionality is equal to self.n_actions from\n",
    "                    the parent class Agent.\n",
    "        '''\n",
    "        return np.clip(-1 + 2 * np.random.rand(self.n_actions), -1, 1)\n",
    "    \n",
    "class ExperienceReplayBuffer(object):\n",
    "    \"\"\" Class used to store a buffer containing experiences of the RL agent.\n",
    "    \"\"\"\n",
    "    def __init__(self, maximum_length):\n",
    "        # Create buffer of maximum length\n",
    "        self.buffer = deque(maxlen=maximum_length)\n",
    "\n",
    "    def append(self, experience):\n",
    "        # Append experience to the buffer\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def __len__(self):\n",
    "        # overload len operator\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def sample_batch(self, n):\n",
    "        \"\"\" Function used to sample experiences from the buffer.\n",
    "            returns 5 lists, each of size n. Returns a list of state, actions,\n",
    "            rewards, next states and done variables.\n",
    "        \"\"\"\n",
    "        # If we try to sample more elements that what are available from the\n",
    "        # buffer we raise an error\n",
    "        if n > len(self.buffer):\n",
    "            raise IndexError('Tried to sample too many elements from the buffer!')\n",
    "\n",
    "        # Sample without replacement the indices of the experiences\n",
    "        # np.random.choice takes 3 parameters: number of elements of the buffer,\n",
    "        # number of elements to sample and replacement.\n",
    "        indices = np.random.choice(\n",
    "            len(self.buffer),\n",
    "            size=n,\n",
    "            replace=False\n",
    "        )\n",
    "\n",
    "        # Using the indices that we just sampled build a list of chosen experiences\n",
    "        batch = [self.buffer[i] for i in indices]\n",
    "\n",
    "        # batch is a list of size n, where each element is an Experience tuple\n",
    "        # of 5 elements. To convert a list of tuples into\n",
    "        # a tuple of list we do zip(*batch). In this case this will return a\n",
    "        # tuple of 5 elements where each element is a list of n elements.\n",
    "        return zip(*batch)\n",
    "\n",
    "### Neural Network ###\n",
    "class actor_net(nn.Module):\n",
    "    \"\"\" Create a feedforward neural network \"\"\"\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "\n",
    "        # Create input layer with ReLU activation\n",
    "        self.input_layer = nn.Linear(input_size, 400)\n",
    "        self.input_layer_activation = nn.ReLU()\n",
    "        \n",
    "        self.input_layer1 = nn.Linear(400, 200)\n",
    "        \n",
    "        self.output_layer= nn.Linear(200, output_size)\n",
    "        \n",
    "        self.output_layer_activation = nn.Tanh()\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Function used to compute the forward pass\n",
    "\n",
    "        # Compute first layer\n",
    "        l1 = self.input_layer(x)\n",
    "        l1 = self.input_layer_activation(l1)\n",
    "        l1 = self.input_layer1(l1)\n",
    "        l1 = self.input_layer_activation(l1)\n",
    "        l1= self.output_layer(l1)\n",
    "        out= self.output_layer_activation(l1)\n",
    "        \n",
    "        \n",
    "        return out\n",
    "    \n",
    "class critic_net(nn.Module):\n",
    "    \"\"\" Create a feedforward neural network \"\"\"\n",
    "    def __init__(self, input_size, output_size, m):\n",
    "        super().__init__()\n",
    "\n",
    "        # Create input layer with ReLU activation\n",
    "        self.input_layer = nn.Linear(input_size, 400)\n",
    "        self.input_layer_activation = nn.ReLU()\n",
    "        \n",
    "        self.input_layer1 = nn.Linear(400+m, 200)\n",
    "        self.output_layer= nn.Linear(200, output_size)\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "    def forward(self, x, a):\n",
    "        # Function used to compute the forward pass\n",
    "\n",
    "        # Compute first layer\n",
    "        l1 = self.input_layer(x)\n",
    "        l1 = self.input_layer_activation(l1)\n",
    "        hidden= torch.cat ([ l1 , a ] , dim =1)\n",
    "        l1 = self.input_layer1(hidden)\n",
    "        l1 = self.input_layer_activation(l1)\n",
    "\n",
    "        out= self.output_layer(l1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        return     \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we fix our parameters for training and we implement it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import and initialize Mountain Car Environment\n",
    "env = gym.make('LunarLanderContinuous-v2')\n",
    "env.reset()\n",
    "\n",
    "# Parameters\n",
    "N_episodes = 300              # Number of episodes to run for training\n",
    "discount_factor = 0.99         # Value of gamma\n",
    "n_ep_running_average = 50      # Running average of 50 episodes\n",
    "n_actions = len(env.action_space.high)               # Number of available actions\n",
    "dim_state = len(env.observation_space.high)  # State dimensionality\n",
    "mu=0.15\n",
    "sigma=0.2\n",
    "lr_actor=5e-5\n",
    "lr_critic=5e-4\n",
    "L=50000\n",
    "tau=1e-3\n",
    "N=64\n",
    "d=2\n",
    "max_norm=1\n",
    "\n",
    "# Reward\n",
    "episode_reward_list = []  # Used to save episodes reward\n",
    "episode_number_of_steps = []\n",
    "\n",
    "### Create Experience replay buffer ###\n",
    "buffer = ExperienceReplayBuffer(maximum_length=L)\n",
    "\n",
    "### Filling up Buffer with Random experiences\n",
    "agent = RandomAgent(n_actions)\n",
    "\n",
    "for i in range(L):\n",
    "    # Reset enviroment data and initialize variables\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "\n",
    "    while not done:\n",
    "        # Take a random action\n",
    "        action = agent.forward(state)\n",
    "\n",
    "        # Get next state and reward.  The done variable\n",
    "        # will be True if you reached the goal position,\n",
    "        # False otherwise\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        exp = Experience(state, action, reward, next_state, done)\n",
    "\n",
    "        buffer.append(exp)\n",
    "\n",
    "        # Update state for next iteration\n",
    "        state = next_state\n",
    "\n",
    "    # Close environment\n",
    "    env.close()\n",
    "\n",
    "\n",
    "### Create network ###\n",
    "\n",
    "actor = actor_net(input_size= dim_state, output_size=n_actions)\n",
    "critic = critic_net(input_size= dim_state, output_size=1 , m=n_actions)\n",
    "\n",
    "\n",
    "target_actor = actor_net(input_size= dim_state, output_size=n_actions)\n",
    "target_critic = critic_net(input_size= dim_state, output_size=1 , m=n_actions)\n",
    "\n",
    "\n",
    "### Create optimizer ###\n",
    "optimizer_actor = optim.Adam(actor.parameters(), lr=lr_actor)\n",
    "optimizer_critic = optim.Adam(critic.parameters(), lr=lr_critic)\n",
    "\n",
    "### PLAY ENVIRONMENT ###\n",
    "# The next while loop plays 5 episode of the environment\n",
    "\n",
    "\n",
    "\n",
    "max_score= -1000\n",
    "avg= -1000\n",
    "#Steps to update target network\n",
    "steps=0\n",
    "\n",
    "# Training process\n",
    "EPISODES = trange(N_episodes, desc='Episode: ', leave=True)\n",
    "\n",
    "for episode in EPISODES:\n",
    "\n",
    "    total_episode_reward=0\n",
    "    state = env.reset()                    # Reset environment, returns\n",
    "    noise=0                                       # initial state\n",
    "    done = False                           # Boolean variable used to indicate\n",
    "\n",
    "    t=0\n",
    "    while not done:\n",
    "        #env.render()                       # Render the environment, remove this\n",
    "                                           # line if you run on Google Colab\n",
    "        # Create state tensor, remember to use single precision (torch.float32)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "        \n",
    "            #Ornstein-Uhlenbeck process as a noise\n",
    "            noise= -mu*noise+ np.random.normal(0, sigma, n_actions)\n",
    "            \n",
    "            \n",
    "\n",
    "            noise_tensor= torch.tensor(noise,\n",
    "                                        requires_grad=False,\n",
    "                                        dtype=torch.float32)\n",
    "\n",
    "            \n",
    "            actor.eval()\n",
    "            #Take perturbated action using actor network\n",
    "            action = actor(torch.tensor(state,\n",
    "                                requires_grad=False,\n",
    "                                dtype=torch.float32)) \n",
    "            \n",
    "            action= action + noise_tensor\n",
    "\n",
    "            action= np.clip(action, -1, 1)\n",
    "\n",
    "        \n",
    "\n",
    "        # The next line takes permits you to take an action in the RL environment\n",
    "        # env.step(action) returns 4 variables:\n",
    "        # (1) next state; (2) reward; (3) done variable; (4) additional stuff\n",
    "        next_state, reward, done, _ = env.step(action.detach().numpy())\n",
    "\n",
    "        # Update episode reward\n",
    "        total_episode_reward += reward\n",
    "\n",
    "        # Append experience to the buffer\n",
    "        exp = Experience(state, action, reward, next_state, done)\n",
    "\n",
    "        buffer.append(exp)\n",
    "        \n",
    "        \n",
    "        # Update state for next iteration\n",
    "        state = next_state\n",
    "\n",
    "        \n",
    "        \n",
    "    ### TRAINING ###\n",
    "        # Perform training only if we have more than N elements in the buffer\n",
    "        if len(buffer) >= N:\n",
    "            \n",
    "            \n",
    "            # Sample a batch of N elements\n",
    "            states, actions, rewards, next_states, dones = buffer.sample_batch(n=N)\n",
    "            \n",
    "            target_critic.eval()\n",
    "            target_actor.eval()\n",
    "            actor.train()\n",
    "            critic.train()\n",
    "            with torch.no_grad():\n",
    "                #target values\n",
    "                next_s=torch.tensor(next_states,\n",
    "                                requires_grad=False,\n",
    "                                dtype=torch.float32)\n",
    "\n",
    "                #Compute actor policy results\n",
    "                input_targets=target_actor(next_s)\n",
    "\n",
    "\n",
    "                #target values\n",
    "                target_values=target_critic(next_s, input_targets)\n",
    "\n",
    "\n",
    "                rewards= torch.tensor(rewards, requires_grad=False,dtype=torch.float32)\n",
    "                rewards=torch.reshape(rewards, target_values.shape)\n",
    "\n",
    "                dones=  torch.tensor(dones, requires_grad=False,dtype=torch.float32)\n",
    "                dones=torch.reshape(dones, target_values.shape)\n",
    "\n",
    "                #Compute target values\n",
    "                target_values= rewards + discount_factor * (1-dones) * (target_values)\n",
    "\n",
    "            #concatenated input values\n",
    "            \n",
    "            s= torch.tensor(states,\n",
    "                            requires_grad=True,\n",
    "                            dtype=torch.float32)\n",
    "            \n",
    "            actions_list=[]\n",
    "            for a in actions:\n",
    "                actions_list.append(list(a))\n",
    "            \n",
    "            a= torch.tensor(actions_list,\n",
    "                            requires_grad=False,\n",
    "                            dtype=torch.float32)\n",
    "            \n",
    "            \n",
    "            #Compute output using actor network\n",
    "            values= critic(s, a)\n",
    "            \n",
    "            # Compute loss function\n",
    "            loss_critic = nn.functional.mse_loss(\n",
    "                            values, target_values.detach())\n",
    "            \n",
    "            \n",
    "            optimizer_critic.zero_grad()\n",
    "            \n",
    "            # Compute gradient\n",
    "            loss_critic.backward()\n",
    "\n",
    "            # Clip gradient norm to 1\n",
    "            nn.utils.clip_grad_norm_(critic.parameters(), max_norm=max_norm)\n",
    "\n",
    "            # Perform backward pass (backpropagation)\n",
    "            optimizer_critic.step()\n",
    "\n",
    "            \n",
    "\n",
    "            if t%d==0:\n",
    "                \n",
    "                \n",
    "                actor_input= torch.tensor(states,\n",
    "                            requires_grad=True,\n",
    "                            dtype=torch.float32)\n",
    "                \n",
    "                actor_output=actor(actor_input)\n",
    "                \n",
    "                \n",
    "                states_actor= torch.tensor(states,\n",
    "                            requires_grad=False,\n",
    "                            dtype=torch.float32)\n",
    "                \n",
    "                \n",
    "                results_actor= critic(states_actor.detach(), actor_output)\n",
    "                \n",
    "                loss_actor= -torch.mean(results_actor)\n",
    "                \n",
    "                \n",
    "                optimizer_actor.zero_grad()\n",
    "\n",
    "                loss_actor.backward()\n",
    "                \n",
    "                nn.utils.clip_grad_norm_(actor.parameters(), max_norm=max_norm)\n",
    "                \n",
    "                optimizer_actor.step()\n",
    "                \n",
    "                target_critic= soft_updates(critic, target_critic, tau)\n",
    "                target_actor= soft_updates(actor, target_actor, tau)\n",
    "                \n",
    "                         \n",
    "\n",
    "        t+=1\n",
    "    \n",
    "    #Add rewards and number of steps\n",
    "    episode_reward_list.append(total_episode_reward)\n",
    "    episode_number_of_steps.append(t)\n",
    "    EPISODES.set_description(\n",
    "        \"Episode {} - Reward/Steps: {:.1f}/{} - Avg. Reward/Steps: {:.1f}/{}\".format(\n",
    "        episode, total_episode_reward, t,\n",
    "        running_average(episode_reward_list, n_ep_running_average)[-1],\n",
    "        running_average(episode_number_of_steps, n_ep_running_average)[-1]))\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# Close all the windows\n",
    "env.close()\n",
    "\n",
    "\n",
    "# Plot Rewards and steps\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(16, 9))\n",
    "ax[0].plot([i for i in range(1, N_episodes+1)], episode_reward_list, label='Episode reward')\n",
    "ax[0].plot([i for i in range(1, N_episodes+1)], running_average(\n",
    "    episode_reward_list, n_ep_running_average), label='Avg. episode reward')\n",
    "ax[0].set_xlabel('Episodes')\n",
    "ax[0].set_ylabel('Total reward')\n",
    "ax[0].set_title('Total Reward vs Episodes')\n",
    "ax[0].legend()\n",
    "ax[0].grid(alpha=0.3)\n",
    "\n",
    "ax[1].plot([i for i in range(1, N_episodes+1)], episode_number_of_steps, label='Steps per episode')\n",
    "ax[1].plot([i for i in range(1, N_episodes+1)], running_average(\n",
    "    episode_number_of_steps, n_ep_running_average), label='Avg. number of steps per episode')\n",
    "ax[1].set_xlabel('Episodes')\n",
    "ax[1].set_ylabel('Total number of steps')\n",
    "ax[1].set_title('Total number of steps vs Episodes')\n",
    "ax[1].legend()\n",
    "ax[1].grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Solution:\n",
    "\n",
    "Here we check our solution for the problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load packages\n",
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "from tqdm import trange\n",
    "\n",
    "def running_average(x, N):\n",
    "    ''' Function used to compute the running average\n",
    "        of the last N elements of a vector x\n",
    "    '''\n",
    "    if len(x) >= N:\n",
    "        y = np.copy(x)\n",
    "        y[N-1:] = np.convolve(x, np.ones((N, )) / N, mode='valid')\n",
    "    else:\n",
    "        y = np.zeros_like(x)\n",
    "    return y\n",
    "\n",
    "# Load model\n",
    "try:\n",
    "    model = actor\n",
    "    print('Network model: {}'.format(model))\n",
    "except:\n",
    "    print('File neural-network-2-actor.pth not found!')\n",
    "    exit(-1)\n",
    "\n",
    "# Import and initialize Mountain Car Environment\n",
    "env = gym.make('LunarLanderContinuous-v2')\n",
    "env.reset()\n",
    "\n",
    "# Parameters\n",
    "N_EPISODES = 50            # Number of episodes to run for trainings\n",
    "CONFIDENCE_PASS = 125\n",
    "\n",
    "# Reward\n",
    "episode_reward_list = []  # Used to store episodes reward\n",
    "\n",
    "# Simulate episodes\n",
    "print('Checking solution...')\n",
    "EPISODES = trange(N_EPISODES, desc='Episode: ', leave=True)\n",
    "for i in EPISODES:\n",
    "    EPISODES.set_description(\"Episode {}\".format(i))\n",
    "    # Reset enviroment data\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    total_episode_reward = 0.\n",
    "    while not done:\n",
    "        # Get next state and reward.  The done variable\n",
    "        # will be True if you reached the goal position,\n",
    "        # False otherwise\n",
    "        action = model(torch.tensor([state]))[0]\n",
    "        next_state, reward, done, _ = env.step(action.detach().numpy())\n",
    "\n",
    "        # Update episode reward\n",
    "        total_episode_reward += reward\n",
    "\n",
    "        # Update state for next iteration\n",
    "        state = next_state\n",
    "\n",
    "    # Append episode reward\n",
    "    episode_reward_list.append(total_episode_reward)\n",
    "\n",
    "    # Close environment\n",
    "    env.close()\n",
    "\n",
    "avg_reward = np.mean(episode_reward_list)\n",
    "confidence = np.std(episode_reward_list) * 1.96 / np.sqrt(N_EPISODES)\n",
    "\n",
    "\n",
    "print('Policy achieves an average total reward of {:.1f} +/- {:.1f} with confidence 95%.'.format(\n",
    "                avg_reward,\n",
    "                confidence))\n",
    "\n",
    "if avg_reward - confidence >= CONFIDENCE_PASS:\n",
    "    print('Your policy passed the test!')\n",
    "else:\n",
    "    print(\"Your policy did not pass the test! The average reward of your policy needs to be greater than {} with 95% confidence\".format(CONFIDENCE_PASS))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
