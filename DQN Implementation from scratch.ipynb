{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN Implementation to solve \"Lunar Lander\" Discrete version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import deque, namedtuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "# Load packages\n",
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import trange\n",
    "import random\n",
    "\n",
    "def running_average(x, N):\n",
    "    ''' Function used to compute the running average\n",
    "        of the last N elements of a vector x\n",
    "    '''\n",
    "    if len(x) >= N:\n",
    "        y = np.copy(x)\n",
    "        y[N-1:] = np.convolve(x, np.ones((N, )) / N, mode='valid')\n",
    "    else:\n",
    "        y = np.zeros_like(x)\n",
    "    return y\n",
    "\n",
    "\n",
    "class RandomAgent(Agent):\n",
    "    ''' Agent taking actions uniformly at random, child of the class Agent'''\n",
    "    def __init__(self, n_actions: int):\n",
    "        super(RandomAgent, self).__init__(n_actions)\n",
    "\n",
    "    def forward(self, state: np.ndarray) -> int:\n",
    "        ''' Compute an action uniformly at random across n_actions possible\n",
    "            choices\n",
    "\n",
    "            Returns:\n",
    "                action (int): the random action\n",
    "        '''\n",
    "        self.last_action = np.random.randint(0, self.n_actions)\n",
    "        return self.last_action\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we define useful classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experience = namedtuple('Experience',\n",
    "                        ['state', 'action', 'reward', 'next_state', 'done'])\n",
    "\n",
    "\n",
    "class RandomAgent(Agent):\n",
    "    ''' Agent taking actions uniformly at random, child of the class Agent'''\n",
    "    def __init__(self, n_actions: int):\n",
    "        super(RandomAgent, self).__init__(n_actions)\n",
    "\n",
    "    def forward(self, state: np.ndarray) -> int:\n",
    "        ''' Compute an action uniformly at random across n_actions possible\n",
    "            choices\n",
    "\n",
    "            Returns:\n",
    "                action (int): the random action\n",
    "        '''\n",
    "        self.last_action = np.random.randint(0, self.n_actions)\n",
    "        return self.last_action\n",
    "\n",
    "    \n",
    "    \n",
    "class ExperienceReplayBuffer(object):\n",
    "    \"\"\" Class used to store a buffer containing experiences of the RL agent.\n",
    "    \"\"\"\n",
    "    def __init__(self, maximum_length):\n",
    "        # Create buffer of maximum length\n",
    "        self.buffer = deque(maxlen=maximum_length)\n",
    "\n",
    "    def append(self, experience):\n",
    "        # Append experience to the buffer\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def __len__(self):\n",
    "        # overload len operator\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def sample_batch(self, n):\n",
    "        \"\"\" Function used to sample experiences from the buffer.\n",
    "            returns 5 lists, each of size n. Returns a list of state, actions,\n",
    "            rewards, next states and done variables.\n",
    "        \"\"\"\n",
    "        # If we try to sample more elements that what are available from the\n",
    "        # buffer we raise an error\n",
    "        if n > len(self.buffer):\n",
    "            raise IndexError('Tried to sample too many elements from the buffer!')\n",
    "\n",
    "        # Sample without replacement the indices of the experiences\n",
    "        # np.random.choice takes 3 parameters: number of elements of the buffer,\n",
    "        # number of elements to sample and replacement.\n",
    "        indices = np.random.choice(\n",
    "            len(self.buffer),\n",
    "            size=n,\n",
    "            replace=False\n",
    "        )\n",
    "\n",
    "        # Using the indices that we just sampled build a list of chosen experiences\n",
    "        batch = [self.buffer[i] for i in indices]\n",
    "\n",
    "        # batch is a list of size n, where each element is an Experience tuple\n",
    "        # of 5 elements. To convert a list of tuples into\n",
    "        # a tuple of list we do zip(*batch). In this case this will return a\n",
    "        # tuple of 5 elements where each element is a list of n elements.\n",
    "        return zip(*batch)\n",
    "\n",
    "### Neural Network ###\n",
    "class Agent(nn.Module):\n",
    "    \"\"\" Create a feedforward neural network \"\"\"\n",
    "    def __init__(self, input_size, output_size, hidden_size=124):\n",
    "        super().__init__()\n",
    "\n",
    "        # Create input layer with ReLU activation\n",
    "        self.input_layer = nn.Linear(input_size,hidden_size)\n",
    "        self.input_layer_activation = nn.ReLU()\n",
    "        self.input_layer1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.output_layer= nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        \n",
    "\n",
    "        # Create output layer\n",
    "        #self.output_layer = nn.Linear(8, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Function used to compute the forward pass\n",
    "\n",
    "        # Compute first layer\n",
    "        l1 = self.input_layer(x)\n",
    "        l1 = self.input_layer_activation(l1)\n",
    "        #l1 = self.input_layer1(l1)\n",
    "        #l1 = self.input_layer_activation(l1)\n",
    "        out= self.output_layer(l1)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "    \n",
    "def epsilon_decay(episode,Z, eps_max=0.99, eps_min=0.05, method=\"linear\"):\n",
    "    \n",
    "    if method=='linear':\n",
    "        \n",
    "        eps= eps_max-((eps_max-eps_min)*(episode-1))/(Z-1)\n",
    "        \n",
    "        return max(eps_min, eps)\n",
    "    \n",
    "    if method==\"exp\":\n",
    "        eps= eps_max* ((eps_min/eps_max)**((episode-1)/(Z-1)))\n",
    "        \n",
    "        return max(eps_min, eps)\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we implement and execute the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model= Agent(input_size=dim_state, output_size=n_actions)\n",
    "torch.save(best_model.state_dict(), \"best_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CREATE RL ENVIRONMENT ###\n",
    "env = gym.make('LunarLander-v2')\n",
    "env.reset()\n",
    "\n",
    "\n",
    "# Parameters\n",
    "N_episodes = 620  # Number of episodes\n",
    "discount_factor = 1.0  # Value of the discount factor\n",
    "n_ep_running_average = 50  # Running average of 50 episodes\n",
    "n_actions = env.action_space.n  # Number of available actions\n",
    "dim_state = len(env.observation_space.high)  # State dimensionality\n",
    "L = 29025  # Size of the Buffer\n",
    "N = 38  # Size of batch\n",
    "C = int(L / N)  # Frequency of updates\n",
    "# We will use these variables to compute the average episodic reward and\n",
    "# the average number of steps per episode\n",
    "\n",
    "\n",
    "hidden_size = 124\n",
    "method = \"linear\"\n",
    "lr = 0.00006\n",
    "max_norm = 1.846938775510204\n",
    "\n",
    "episode_reward_list = []  # this list contains the total reward per episode\n",
    "episode_number_of_steps = []  # this list contains the number of steps per episode\n",
    "\n",
    "### Create Experience replay buffer ###\n",
    "buffer = ExperienceReplayBuffer(maximum_length=L)\n",
    "\n",
    "best_model= Agent(input_size=dim_state, output_size=n_actions, hidden_size=hidden_size)\n",
    "torch.save(best_model.state_dict(), \"best_model.pth\")\n",
    "\n",
    "### Filling up Buffer with Random experiences\n",
    "agent = RandomAgent(n_actions)\n",
    "\n",
    "for i in range(N):\n",
    "    # Reset enviroment data and initialize variables\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "\n",
    "    while not done:\n",
    "        # Take a random action\n",
    "        action = agent.forward(state)\n",
    "\n",
    "        # Get next state and reward.  The done variable\n",
    "        # will be True if you reached the goal position,\n",
    "        # False otherwise\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        exp = Experience(state, action, reward, next_state, done)\n",
    "\n",
    "        buffer.append(exp)\n",
    "\n",
    "        # Update state for next iteration\n",
    "        state = next_state\n",
    "\n",
    "    # Close environment\n",
    "    env.close()\n",
    "\n",
    "### Create network ###\n",
    "network = Agent(input_size=dim_state, output_size=n_actions, hidden_size=hidden_size)\n",
    "\n",
    "target_network = Agent(input_size=dim_state, output_size=n_actions, hidden_size=hidden_size)\n",
    "target_network.load_state_dict(network.state_dict())\n",
    "\n",
    "### Create optimizer ###\n",
    "optimizer = optim.Adam(network.parameters(), lr=lr)\n",
    "\n",
    "### PLAY ENVIRONMENT ###\n",
    "# The next while loop plays 5 episode of the environment\n",
    "\n",
    "\n",
    "max_score = -1000\n",
    "avg = -1000\n",
    "# Steps to update target network\n",
    "steps = 0\n",
    "\n",
    "EPISODES = trange(N_episodes)\n",
    "\n",
    "for episode in EPISODES:\n",
    "\n",
    "    total_episode_reward = 0\n",
    "    state = env.reset()  # Reset environment, returns\n",
    "    # initial state\n",
    "    done = False  # Boolean variable used to indicate\n",
    "\n",
    "    # define greedy epsilon\n",
    "    Z = int(0.93 * N_episodes)\n",
    "\n",
    "    epsilon = epsilon_decay(episode, Z, eps_max=0.9, eps_min=0.05, method=method)\n",
    "\n",
    "    # if an episode terminated\n",
    "\n",
    "    t = 0\n",
    "    while not done:\n",
    "        # env.render()                       # Render the environment, remove this\n",
    "        # line if you run on Google Colab\n",
    "        # Create state tensor, remember to use single precision (torch.float32)\n",
    "        state_tensor = torch.tensor([state],\n",
    "                                    requires_grad=False,\n",
    "                                    dtype=torch.float32)\n",
    "\n",
    "        explore = random.random() < epsilon\n",
    "\n",
    "        if explore:\n",
    "            action = np.random.randint(0, n_actions)\n",
    "\n",
    "\n",
    "        else:\n",
    "            network.eval()\n",
    "            # Compute output of the network\n",
    "            with torch.no_grad():\n",
    "                values = network(state_tensor)\n",
    "                action = values.max(1)[1].item()\n",
    "\n",
    "        # The next line takes permits you to take an action in the RL environment\n",
    "        # env.step(action) returns 4 variables:\n",
    "        # (1) next state; (2) reward; (3) done variable; (4) additional stuff\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # Update episode reward\n",
    "        total_episode_reward += reward\n",
    "\n",
    "        # Append experience to the buffer\n",
    "        exp = Experience(state, action, reward, next_state, done)\n",
    "\n",
    "        buffer.append(exp)\n",
    "\n",
    "        steps += 1\n",
    "\n",
    "        # Update state for next iteration\n",
    "        state = next_state\n",
    "\n",
    "        ### TRAINING ###\n",
    "        # Perform training only if we have more than 3 elements in the buffer\n",
    "        if len(buffer) >= N:\n",
    "            network.train()\n",
    "            # Sample a batch of N elements\n",
    "            states, actions, rewards, next_states, dones = buffer.sample_batch(n=N)\n",
    "\n",
    "            # Compute output of the network given the states batch\n",
    "            values = network(torch.tensor(states,\n",
    "                                          requires_grad=True,\n",
    "                                          dtype=torch.float32))\n",
    "\n",
    "            actions = torch.tensor(actions,\n",
    "                                   requires_grad=False,\n",
    "                                   dtype=torch.int64)\n",
    "            action_masks = F.one_hot(actions, n_actions)\n",
    "\n",
    "            values = (action_masks * values).sum(dim=-1)  # Computing Q(s,a)\n",
    "\n",
    "            # target values\n",
    "            target_values = target_network(torch.tensor(next_states,\n",
    "                                                        requires_grad=False,\n",
    "                                                        dtype=torch.float32))\n",
    "\n",
    "            target_values = target_values.max(1)[0]  # max Q(si,a) for all the batch when target network is used\n",
    "\n",
    "            rewards = torch.tensor(rewards, requires_grad=False, dtype=torch.float32)\n",
    "\n",
    "            dones = torch.tensor(dones, requires_grad=False, dtype=torch.float32)\n",
    "\n",
    "            # Compute target values\n",
    "            target_values = rewards + discount_factor * (1 - dones) * (target_values)\n",
    "\n",
    "            # Compute loss function\n",
    "            loss = nn.functional.mse_loss(\n",
    "                values, target_values.detach())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            # Compute gradient\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip gradient norm to 1\n",
    "            nn.utils.clip_grad_norm_(network.parameters(), max_norm=max_norm)\n",
    "\n",
    "            # Perform backward pass (backpropagation)\n",
    "            optimizer.step()\n",
    "\n",
    "        if steps % C == 0:\n",
    "            target_network.load_state_dict(network.state_dict())\n",
    "\n",
    "        t += 1\n",
    "\n",
    "    # Add rewards and number of steps\n",
    "    episode_reward_list.append(total_episode_reward)\n",
    "    episode_number_of_steps.append(t)\n",
    "\n",
    "    if episode > 65:\n",
    "        avg = running_average(episode_reward_list[-60:], n_ep_running_average)[-1]\n",
    "\n",
    "    if avg > max_score:\n",
    "        max_score = avg\n",
    "        best_model.load_state_dict(network.state_dict())\n",
    "        torch.save(best_model.state_dict(), \"best_model.pth\")\n",
    "        print(\"In episode {}, best score is {}\".format(episode, avg))\n",
    "\n",
    "    EPISODES.set_description(\n",
    "        \"Episode {} - Reward/Steps: {:.1f}/{} - Avg. Reward/Steps: {:.1f}/{}\".format(\n",
    "            i, total_episode_reward, t,\n",
    "            running_average(episode_reward_list, n_ep_running_average)[-1],\n",
    "            running_average(episode_number_of_steps, n_ep_running_average)[-1]))\n",
    "\n",
    "# Close all the windows\n",
    "env.close()\n",
    "\n",
    "# Plot Rewards and steps\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(16, 9))\n",
    "ax[0].plot([i for i in range(1, N_episodes + 1)], episode_reward_list, label='Episode reward')\n",
    "ax[0].plot([i for i in range(1, N_episodes + 1)], running_average(\n",
    "    episode_reward_list, n_ep_running_average), label='Avg. episode reward')\n",
    "ax[0].set_xlabel('Episodes')\n",
    "ax[0].set_ylabel('Total reward')\n",
    "ax[0].set_title('Total Reward vs Episodes')\n",
    "ax[0].legend()\n",
    "ax[0].grid(alpha=0.3)\n",
    "\n",
    "ax[1].plot([i for i in range(1, N_episodes + 1)], episode_number_of_steps, label='Steps per episode')\n",
    "ax[1].plot([i for i in range(1, N_episodes + 1)], running_average(\n",
    "    episode_number_of_steps, n_ep_running_average), label='Avg. number of steps per episode')\n",
    "ax[1].set_xlabel('Episodes')\n",
    "ax[1].set_ylabel('Total number of steps')\n",
    "ax[1].set_title('Total number of steps vs Episodes')\n",
    "ax[1].legend()\n",
    "ax[1].grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "################################################################################\n",
    "\n",
    "print('Done.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random search\n",
    "\n",
    "It is always tricky to tune this kind of model. Below, we implemented a random search over hyper-parameters that has managed to solve the problem in 100 iteration (200+ average reward over 50 episodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function(parameters, iteration):\n",
    "    \n",
    "    \n",
    "    N_episodes= parameters[\"N_episodes\"]\n",
    "    discount_factor= parameters[\"discount_factor\"]\n",
    "    L= parameters[\"L\"]\n",
    "    N= parameters[\"N\"]\n",
    "    C= int(L/N) \n",
    "    hidden_size= parameters[\"hidden_size\"]\n",
    "    method = parameters[\"method\"]\n",
    "    lr= parameters[\"lr\"]\n",
    "    max_norm= parameters[\"max_norm\"]\n",
    "    \n",
    "    episode_reward_list = []       # this list contains the total reward per episode\n",
    "    episode_number_of_steps = []   # this list contains the number of steps per episode\n",
    "\n",
    "    ### Create Experience replay buffer ###\n",
    "    buffer = ExperienceReplayBuffer(maximum_length=L)\n",
    "\n",
    "    ### Filling up Buffer with Random experiences\n",
    "    agent = RandomAgent(n_actions)\n",
    "\n",
    "    for i in range(N):\n",
    "        # Reset enviroment data and initialize variables\n",
    "        done = False\n",
    "        state = env.reset()\n",
    "\n",
    "        while not done:\n",
    "            # Take a random action\n",
    "            action = agent.forward(state)\n",
    "\n",
    "            # Get next state and reward.  The done variable\n",
    "            # will be True if you reached the goal position,\n",
    "            # False otherwise\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            exp = Experience(state, action, reward, next_state, done)\n",
    "\n",
    "            buffer.append(exp)\n",
    "\n",
    "            # Update state for next iteration\n",
    "            state = next_state\n",
    "\n",
    "        # Close environment\n",
    "        env.close()\n",
    "\n",
    "\n",
    "    ### Create network ###\n",
    "    network = Agent(input_size=dim_state, output_size=n_actions)\n",
    "\n",
    "    target_network= Agent(input_size=dim_state, output_size=n_actions)\n",
    "    target_network.load_state_dict(network.state_dict())\n",
    "\n",
    "    ### Create optimizer ###\n",
    "    optimizer = optim.Adam(network.parameters(), lr=lr)\n",
    "\n",
    "    ### PLAY ENVIRONMENT ###\n",
    "    # The next while loop plays 5 episode of the environment\n",
    "\n",
    "\n",
    "    \n",
    "    max_score= -1000\n",
    "    avg= -1000\n",
    "    #Steps to update target network\n",
    "    steps=0\n",
    "    \n",
    "    #Initialize best model\n",
    "    best_model= Agent(input_size=dim_state, output_size=n_actions)\n",
    "\n",
    "\n",
    "    for episode in range(N_episodes):\n",
    "\n",
    "        total_episode_reward=0\n",
    "        state = env.reset()                    # Reset environment, returns\n",
    "                                               # initial state\n",
    "        done = False                           # Boolean variable used to indicate\n",
    "\n",
    "        #define greedy epsilon\n",
    "        Z= int(0.93*N_episodes)\n",
    "\n",
    "        epsilon= epsilon_decay(episode,Z, eps_max=0.9, eps_min=0.05, method=method)\n",
    "\n",
    "\n",
    "        # if an episode terminated\n",
    "\n",
    "        t=0\n",
    "        while not done:\n",
    "            #env.render()                       # Render the environment, remove this\n",
    "                                               # line if you run on Google Colab\n",
    "            # Create state tensor, remember to use single precision (torch.float32)\n",
    "            state_tensor = torch.tensor([state],\n",
    "                                        requires_grad=False,\n",
    "                                        dtype=torch.float32)\n",
    "\n",
    "\n",
    "            explore = random.random() < epsilon\n",
    "\n",
    "            if explore:\n",
    "                action= np.random.randint(0, n_actions)\n",
    "\n",
    "\n",
    "            else:\n",
    "                # Compute output of the network\n",
    "                with torch.no_grad():\n",
    "                    values = network(state_tensor)\n",
    "                    action = values.max(1)[1].item()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # The next line takes permits you to take an action in the RL environment\n",
    "            # env.step(action) returns 4 variables:\n",
    "            # (1) next state; (2) reward; (3) done variable; (4) additional stuff\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            # Update episode reward\n",
    "            total_episode_reward += reward\n",
    "\n",
    "            # Append experience to the buffer\n",
    "            exp = Experience(state, action, reward, next_state, done)\n",
    "\n",
    "            buffer.append(exp)\n",
    "            \n",
    "            # Update state for next iteration\n",
    "            state = next_state\n",
    "\n",
    "            ### TRAINING ###\n",
    "            # Perform training only if we have more than 3 elements in the buffer\n",
    "            if len(buffer) >= N:\n",
    "                # Sample a batch of N elements\n",
    "                states, actions, rewards, next_states, dones = buffer.sample_batch(n=N)\n",
    "\n",
    "                # Training process, set gradients to 0\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Compute output of the network given the states batch\n",
    "                values = network(torch.tensor(states,\n",
    "                                requires_grad=True,\n",
    "                                dtype=torch.float32))\n",
    "\n",
    "                actions= torch.tensor(actions,\n",
    "                                requires_grad=False,\n",
    "                                dtype=torch.int64)\n",
    "                action_masks = F.one_hot(actions, n_actions)\n",
    "\n",
    "                values = (action_masks * values).sum(dim=-1)  #Computing Q(s,a) \n",
    "\n",
    "                #target values\n",
    "                target_values=target_network(torch.tensor(next_states,\n",
    "                                requires_grad=False,\n",
    "                                dtype=torch.float32))\n",
    "\n",
    "                target_values= target_values.max(1)[0] #max Q(si,a) for all the batch when target network is used\n",
    "\n",
    "                rewards= torch.tensor(rewards, requires_grad=False,dtype=torch.float32)\n",
    "\n",
    "                dones=  torch.tensor(dones, requires_grad=False,dtype=torch.float32)\n",
    "\n",
    "\n",
    "                #Compute target values\n",
    "                target_values= rewards + discount_factor * (1-dones) * (target_values)\n",
    "\n",
    "                # Compute loss function\n",
    "                loss = nn.functional.mse_loss(\n",
    "                                values, target_values.detach())\n",
    "\n",
    "                # Compute gradient\n",
    "                loss.backward()\n",
    "\n",
    "                # Clip gradient norm to 1\n",
    "                nn.utils.clip_grad_norm_(network.parameters(), max_norm=max_norm)\n",
    "\n",
    "                # Perform backward pass (backpropagation)\n",
    "                optimizer.step()\n",
    "\n",
    "                steps+=1\n",
    "\n",
    "\n",
    "            if steps%C==0:\n",
    "                target_network.load_state_dict(network.state_dict())\n",
    "\n",
    "            t+=1\n",
    "\n",
    "        #Add rewards and number of steps\n",
    "        episode_reward_list.append(total_episode_reward)\n",
    "        episode_number_of_steps.append(t)\n",
    "        \n",
    "        if episode>65:\n",
    "            avg=running_average(episode_reward_list[-60:], n_ep_running_average)[-1]\n",
    "        \n",
    "        \n",
    "        if avg>max_score:\n",
    "            max_score =  avg\n",
    "            best_model.load_state_dict(network.state_dict())\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "    # Write to the csv file ('a' means append)\n",
    "    of_connection = open(out_file, 'a')\n",
    "    writer = csv.writer(of_connection)\n",
    "    writer.writerow([max_score, parameters, iteration])\n",
    "\n",
    "    # Close all the windows\n",
    "    env.close()\n",
    "    \n",
    "    return [max_score, parameters, iteration, best_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import csv\n",
    "out_file = 'parameters_dqn.csv'\n",
    "of_connection = open(out_file, 'w')\n",
    "writer = csv.writer(of_connection)\n",
    "\n",
    "# Write the headers to the file\n",
    "writer.writerow(['reward', 'params', 'iteration'])\n",
    "of_connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "\n",
    "param_grid = {\n",
    "    'N_episodes': list(range(100, 1000)),\n",
    "    'discount_factor': list(np.linspace(0.2, 1)),\n",
    "    'L': list(range(20000, 30000)),\n",
    "    'N': list(range(4, 124)),\n",
    "    'hidden_size': list(range(8, 124)),\n",
    "    'method': [\"linear\",\"exp\"],\n",
    "    'lr': list(np.linspace(0.00001, 0.0001)),\n",
    "    'max_norm': list(np.linspace(0.5, 2)),\n",
    "    \n",
    "}\n",
    "\n",
    "MAX_EVALS=600\n",
    "# Dataframe to hold cv results\n",
    "random_results = pd.DataFrame(columns = ['reward', 'params', 'iteration'],\n",
    "                       index = list(range(MAX_EVALS)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(50)\n",
    "final_model= Agent(input_size=dim_state, output_size=n_actions)\n",
    "torch.save(final_model.state_dict(), \"best_model.pt\")\n",
    "best=-1000\n",
    "# Iterate through the specified number of evaluations\n",
    "for i in range(MAX_EVALS):\n",
    "    \n",
    "    print(\"Step: {}\".format(i))\n",
    "    # Randomly sample parameters for gbm\n",
    "    params = {key: random.sample(value, 1)[0] for key, value in param_grid.items()}\n",
    "    \n",
    "    print(params)\n",
    "\n",
    "    results_list = function(params, i)\n",
    "    \n",
    "    avg=results_list[0]\n",
    "    \n",
    "    if avg>best:\n",
    "        best=avg\n",
    "        final_model= results_list[-1]\n",
    "        torch.save(final_model.state_dict(), \"best_model.pt\")\n",
    "    \n",
    "    print(results_list[:-1])\n",
    "    \n",
    "    # Add results to next row in dataframe\n",
    "    random_results.loc[i, :] = results_list[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_results.loc[0, :] = results_list[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort results by best validation score\n",
    "random_results.sort_values('reward', ascending = False, inplace = True)\n",
    "random_results.reset_index(inplace = True, drop = True)\n",
    "random_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_results.loc[0, 'params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Rewards and steps\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(16, 9))\n",
    "ax[0].plot([i for i in range(1, N_episodes+1)], episode_reward_list, label='Episode reward')\n",
    "ax[0].plot([i for i in range(1, N_episodes+1)], running_average(\n",
    "    episode_reward_list, n_ep_running_average), label='Avg. episode reward')\n",
    "ax[0].set_xlabel('Episodes')\n",
    "ax[0].set_ylabel('Total reward')\n",
    "ax[0].set_title('Total Reward vs Episodes')\n",
    "ax[0].legend()\n",
    "ax[0].grid(alpha=0.3)\n",
    "\n",
    "\n",
    "ax[1].plot([i for i in range(1, N_episodes+1)], episode_number_of_steps, label='Steps per episode')\n",
    "ax[1].plot([i for i in range(1, N_episodes+1)], running_average(\n",
    "    episode_number_of_steps, n_ep_running_average), label='Avg. number of steps per episode')\n",
    "ax[1].set_xlabel('Episodes')\n",
    "ax[1].set_ylabel('Total number of steps')\n",
    "ax[1].set_title('Total number of steps vs Episodes')\n",
    "ax[1].legend()\n",
    "ax[1].grid(alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "N_EPISODES = 50            # Number of episodes to run for trainings\n",
    "CONFIDENCE_PASS = 50\n",
    "\n",
    "# Reward\n",
    "episode_reward_list = []  # Used to store episodes reward\n",
    "\n",
    "model=best_model\n",
    "# Simulate episodes\n",
    "print('Checking solution...')\n",
    "EPISODES = trange(N_EPISODES, desc='Episode: ', leave=True)\n",
    "for i in EPISODES:\n",
    "    EPISODES.set_description(\"Episode {}\".format(i))\n",
    "    # Reset enviroment data\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    total_episode_reward = 0.\n",
    "    while not done:\n",
    "        # Get next state and reward.  The done variable\n",
    "        # will be True if you reached the goal position,\n",
    "        # False otherwise\n",
    "        q_values = model(torch.tensor([state]))\n",
    "        _, action = torch.max(q_values, axis=1)\n",
    "        next_state, reward, done, _ = env.step(action.item())\n",
    "\n",
    "        # Update episode reward\n",
    "        total_episode_reward += reward\n",
    "\n",
    "        # Update state for next iteration\n",
    "        state = next_state\n",
    "\n",
    "    # Append episode reward\n",
    "    episode_reward_list.append(total_episode_reward)\n",
    "\n",
    "    # Close environment\n",
    "    env.close()\n",
    "\n",
    "avg_reward = np.mean(episode_reward_list)\n",
    "confidence = np.std(episode_reward_list) * 1.96 / np.sqrt(N_EPISODES)\n",
    "\n",
    "\n",
    "print('Policy achieves an average total reward of {:.1f} +/- {:.1f} with confidence 95%.'.format(\n",
    "                avg_reward,\n",
    "                confidence))\n",
    "\n",
    "if avg_reward - confidence >= CONFIDENCE_PASS:\n",
    "    print('Your policy passed the test!')\n",
    "else:\n",
    "    print(\"Your policy did not pass the test! The average reward of your policy needs to be greater than {} with 95% confidence\".format(CONFIDENCE_PASS))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
